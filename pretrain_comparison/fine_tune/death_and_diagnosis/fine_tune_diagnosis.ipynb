{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd().split('/pretrain_comparison/fine_tune/death_and_diagnosis')[0] + '/pretrain_comparison')\n",
    "from fine_tune.models.model import SleepEventLSTMClassifier\n",
    "from fine_tune.models.dataset import SleepEventClassificationDataset, finetune_collate_fn\n",
    "from fine_tune.utils import *\n",
    "from comparison.utils import *\n",
    "import json\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.getcwd().split('/pretrain_comparison/fine_tune/death_and_diagnosis')[0] + '/pretrain_comparison/fine_tune/config_fine_tune.yaml'\n",
    "config = load_data(config_path)\n",
    "config[\"batch_size\"] = config[\"batch_size\"] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class SleepEventClassificationDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 channel_groups=None,\n",
    "                 hdf5_paths=[],\n",
    "                 split=\"train\",\n",
    "                 pretrain_type = \"MAE\",\n",
    "                 specific_files = None):\n",
    "\n",
    "        self.config = config\n",
    "        #self.max_channels = self.config[\"max_channels\"]\n",
    "        self.context = int(self.config[\"context\"])\n",
    "        self.channel_like = self.config[\"channel_like\"]\n",
    "\n",
    "        #diagnosis, death, and demographics\n",
    "        self.df_demographics = pd.read_csv(config['demographics_labels_path'])\n",
    "        self.df_diagnosis_presence = pd.read_csv(os.path.join(config['diagnosis_labels_path'], 'is_event.csv'))\n",
    "        self.df_diagnosis_time = pd.read_csv(os.path.join(config['diagnosis_labels_path'], 'time_to_event.csv'))\n",
    "        self.df_death_presence = pd.read_csv(os.path.join(config['death_labels_path'], 'is_event.csv'), usecols=['Study ID','death'])\n",
    "        self.df_death_time = pd.read_csv(os.path.join(config['death_labels_path'], 'time_to_event.csv'), usecols=['Study ID','death'])\n",
    "        self.df_ahi = pd.read_csv(config['ahi_labels_path'])\n",
    "        self.df_ahi['diagnosis'] = self.df_ahi.ahi.apply(lambda x: 1 if x >= 15 else 0)\n",
    "\n",
    "        unique_study_ids_in_demo_diag_death = set(self.df_demographics['Study ID'].values).intersection(set(self.df_diagnosis_presence['Study ID'].values)).intersection(set(self.df_diagnosis_time['Study ID'].values)).intersection(set(self.df_death_presence['Study ID'].values)).intersection(set(self.df_death_time['Study ID'].values))\n",
    "\n",
    "        labels_path = self.config[\"labels_path\"]\n",
    "        dataset = self.config[\"dataset\"]\n",
    "        dataset = dataset.split(\",\")\n",
    "\n",
    "        label_files = []\n",
    "\n",
    "        for dataset_name in dataset:\n",
    "            label_files += glob.glob(os.path.join(labels_path, dataset_name, \"**\", \"*.csv\"), recursive=True)\n",
    "\n",
    "        # label_files = [label_file for label_file in os.listdir(labels_path) if label_file.endswith(\".csv\")]\n",
    "\n",
    "        hdf5_paths = load_data(config[\"split_path\"])[split]\n",
    "        #print(f'first hdf5_paths: {hdf5_paths[0]}')\n",
    "        #print(f'len hdf5_paths: {len(hdf5_paths)}')\n",
    "        #print(f'first label_files: {label_files[0]}')\n",
    "        #print(f'len label_files: {len(label_files)}')\n",
    "        study_ids = set([os.path.basename(label_file).split(\".\")[0] for label_file in label_files])\n",
    "        #print(f'first study_ids: {list(study_ids)[0]}')\n",
    "        #print(f'len study_ids: {len(study_ids)}')\n",
    "\n",
    "        hdf5_paths = [f for f in hdf5_paths if os.path.exists(f)]\n",
    "        #print(f'len hdf5_paths: {len(hdf5_paths)}')\n",
    "        hdf5_paths = [f for f in hdf5_paths if f.split(\"/\")[-1].split(\".\")[0] in study_ids]\n",
    "        hdf5_paths = [f for f in hdf5_paths if f.split(\"/\")[-1].split(\".\")[0] in unique_study_ids_in_demo_diag_death]\n",
    "        #print(f'len hdf5_paths: {len(hdf5_paths)}')\n",
    "\n",
    "        hdf5_paths_ids = set([os.path.basename(hdf5_path).split(\".\")[0] for hdf5_path in hdf5_paths])\n",
    "        #print(f'first hdf5_paths_ids: {list(hdf5_paths_ids)[0]}')\n",
    "        #print(f'len hdf5_paths_ids: {len(hdf5_paths_ids)}')\n",
    "\n",
    "        hdf5_paths_new = []\n",
    "        #print(f'dataset: {dataset}')\n",
    "        #for dataset_name in dataset:\n",
    "            #hdf5_paths_new += glob.glob(os.path.join(config[\"embedding_path\"], dataset_name, \"**\", \"*.hdf5\"), recursive=True)\n",
    "        hdf5_paths_new += glob.glob(os.path.join(config[\"embedding_path\"], pretrain_type, \"**\", \"*.hdf5\"), recursive=True)\n",
    "        #print(f'first hdf5_paths_new: {hdf5_paths_new[0]}')\n",
    "        \n",
    "        #print(f'len hdf5_paths_new: {len(hdf5_paths_new)}')\n",
    "        \n",
    "        hdf5_paths_new = [item for item in hdf5_paths_new if os.path.basename(item).split(\".\")[0] in hdf5_paths_ids]\n",
    "        #print(f'len hdf5_paths_new: {len(hdf5_paths_new)}')\n",
    "        hdf5_paths = hdf5_paths_new\n",
    "        hdf5_paths = [f for f in hdf5_paths if os.path.exists(f)]\n",
    "        #print(f'len hdf5_paths: {len(hdf5_paths)}')\n",
    "\n",
    "        if config[\"max_files\"]:\n",
    "            hdf5_paths = hdf5_paths[:config[\"max_files\"]]\n",
    "        else:\n",
    "            hdf5_paths = hdf5_paths\n",
    "\n",
    "        labels_dict = {\n",
    "            os.path.basename(item).split(\".\")[0]: item for item in label_files\n",
    "        }\n",
    "        if specific_files:\n",
    "            #print(f'hdf5_paths[0] {hdf5_paths[0]}')\n",
    "            #print(f'specific_files[0] {specific_files[0]}')\n",
    "            \n",
    "            # Extract base names from specific_files (without extension) for proper comparison\n",
    "            specific_files_base = [os.path.splitext(f)[0] for f in specific_files]\n",
    "            \n",
    "            # Filter hdf5_paths to only include files whose base names are in specific_files\n",
    "            hdf5_paths = [f for f in hdf5_paths if os.path.splitext(os.path.basename(f))[0] in specific_files_base]\n",
    "            \n",
    "            #print(f'number of specific_files: {len(hdf5_paths)}')\n",
    "            repeats = max(1024 // len(specific_files), 1)\n",
    "            \n",
    "            # Repeat the hdf5 files\n",
    "            hdf5_paths = [f for f in hdf5_paths for _ in range(repeats)]\n",
    "            #print(f'number of training items per epoch: {len(hdf5_paths)}')\n",
    "        if self.context == -1:\n",
    "            self.index_map = [(path, labels_dict[path.split(\"/\")[-1].split(\".\")[0]], -1) for path in hdf5_paths]\n",
    "        else:\n",
    "            self.index_map = []\n",
    "            loop = tqdm(hdf5_paths[:], total=len(hdf5_paths), desc=f\"Indexing {split} data\")\n",
    "            for hdf5_file_path in loop:\n",
    "                file_prefix = os.path.basename(hdf5_file_path).split(\".\")[0]\n",
    "                with h5py.File(hdf5_file_path, \"r\") as file:\n",
    "                    dataset_names = list(file.keys())[:]\n",
    "                    dataset_name = dataset_names[0]\n",
    "                    dataset_length = file[dataset_name].shape[0]\n",
    "                    for i in range(0, dataset_length, self.context):\n",
    "                        self.index_map.append((hdf5_file_path, labels_dict[file_prefix], i))           \n",
    "            \n",
    "        #logger.info(f\"Number of files in {split} set: {len(hdf5_paths)}\")\n",
    "        #logger.info(f\"Number of files to be processed in {split} set: {len(self.index_map)}\")\n",
    "        self.total_len = len(self.index_map)\n",
    "        self.max_seq_len = config[\"model_params\"][\"max_seq_length\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def get_index_map(self):\n",
    "        return self.index_map\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hdf5_path, label_path, start_index = self.index_map[idx]\n",
    "        labels_df = pd.read_csv(label_path)\n",
    "        y_data = labels_df[\"StageNumber\"].to_numpy()\n",
    "        if self.context != -1:\n",
    "            y_data = y_data[start_index:start_index+self.context]\n",
    "        x_data = []\n",
    "        with h5py.File(hdf5_path, 'r') as hf:\n",
    "            dset_names = list(hf.keys())[:]\n",
    "            for dataset_name in dset_names:\n",
    "                x_data.append(hf[dataset_name][:])\n",
    "        x_data = np.array(x_data)\n",
    "        # Convert x_data to tensor\n",
    "        x_data = torch.tensor(x_data, dtype=torch.float32)\n",
    "        y_data = torch.tensor(y_data, dtype=torch.float32)\n",
    "        min_length = min(x_data.shape[1], len(y_data))\n",
    "        x_data = x_data[:, :min_length, :].squeeze()\n",
    "        y_data = y_data[:min_length]\n",
    "        \n",
    "        #diagnosis, death, and demographics\n",
    "        study_id = os.path.basename(hdf5_path).split(\".\")[0]\n",
    "        try:\n",
    "            diagnosis_presence = torch.tensor(self.df_diagnosis_presence[self.df_diagnosis_presence['Study ID'] == study_id].values[0][1:].astype(np.float32))\n",
    "            diagnosis_time = torch.tensor(self.df_diagnosis_time[self.df_diagnosis_time['Study ID'] == study_id].values[0][1:].astype(np.float32))\n",
    "            death_presence = torch.tensor(self.df_death_presence[self.df_death_presence['Study ID'] == study_id].values[0][1:].astype(np.float32))\n",
    "            death_time = torch.tensor(self.df_death_time[self.df_death_time['Study ID'] == study_id].values[0][1:].astype(np.float32))\n",
    "            age = torch.tensor(self.df_demographics[self.df_demographics['Study ID'] == study_id]['Age at Study Date'].values) / 100\n",
    "            try:\n",
    "                ahi_diagnosis = torch.tensor(self.df_ahi[self.df_ahi['Study ID'] == study_id]['diagnosis'].values)\n",
    "            except:\n",
    "                ahi_diagnosis = torch.tensor([0])\n",
    "        except:\n",
    "            print(f'Study ID {study_id} not found in demographics, diagnosis, or death data')\n",
    "\n",
    "        \n",
    "        return x_data, y_data, self.max_seq_len, hdf5_path, diagnosis_presence, diagnosis_time, death_presence, death_time, age, ahi_diagnosis\n",
    "\n",
    "def finetune_collate_fn(batch):\n",
    "\n",
    "    x_data, y_data, max_seq_len_list, hdf5_path_list, diagnosis_presence, diagnosis_time, death_presence, death_time, age, ahi_diagnosis = zip(*batch)\n",
    "\n",
    "    # padding the temporal as in sleep_event_finetune_full_collate_fn\n",
    "    max_seq_len_temp = max([item.size(0) for item in x_data])\n",
    "    # Determine the max sequence length for padding\n",
    "    if max_seq_len_list[0] is None:\n",
    "        max_seq_len = max_seq_len_temp\n",
    "    else:\n",
    "        max_seq_len = min(max_seq_len_temp, max_seq_len_list[0])\n",
    "    \n",
    "    padded_x_data = []\n",
    "    padded_y_data = []\n",
    "    padded_mask = []\n",
    "    diagnosis_presence_list = []\n",
    "    diagnosis_time_list = []\n",
    "    death_presence_list = []\n",
    "    death_time_list = []\n",
    "    age_list = []\n",
    "    ahi_diagnosis_list = []\n",
    "\n",
    "    for x_item, y_item, diagnosis_presence_item, diagnosis_time_item, death_presence_item, death_time_item, age_item, ahi_diagnosis_item  in zip(x_data, y_data, diagnosis_presence, diagnosis_time, death_presence, death_time, age, ahi_diagnosis):\n",
    "        # Get the shape of x_item\n",
    "        s, e = x_item.size()\n",
    "\n",
    "        s = min(s, max_seq_len)\n",
    "\n",
    "        # Create a padded tensor and a mask tensor for x_data\n",
    "        padded_x_item = torch.zeros((max_seq_len, e))\n",
    "        mask = torch.ones((max_seq_len))\n",
    "\n",
    "        # Copy the actual data to the padded tensor and set the mask for real data\n",
    "        padded_x_item[:s, :e] = x_item[:s, :e]\n",
    "        mask[:s] = 0  # 0 for real data, 1 for padding\n",
    "\n",
    "        # Pad y_data with zeros to match max_seq_len\n",
    "        padded_y_item = torch.zeros(max_seq_len)\n",
    "        padded_y_item[:s] = y_item[:s]\n",
    "\n",
    "        # Append padded items to lists\n",
    "        padded_x_data.append(padded_x_item)\n",
    "        padded_y_data.append(padded_y_item)\n",
    "        padded_mask.append(mask)\n",
    "        diagnosis_presence_list.append(diagnosis_presence_item)\n",
    "        diagnosis_time_list.append(diagnosis_time_item)\n",
    "        death_presence_list.append(death_presence_item)\n",
    "        death_time_list.append(death_time_item)\n",
    "        age_list.append(age_item)\n",
    "        ahi_diagnosis_list.append(ahi_diagnosis_item)\n",
    "\n",
    "\n",
    "\n",
    "    # Stack all tensors into a batch\n",
    "    x_data = torch.stack(padded_x_data)\n",
    "    y_data = torch.stack(padded_y_data)\n",
    "    padded_mask = torch.stack(padded_mask)\n",
    "\n",
    "    diagnosis_presence = torch.stack(diagnosis_presence_list)\n",
    "    diagnosis_time = torch.stack(diagnosis_time_list)\n",
    "    death_presence = torch.tensor(death_presence_list).unsqueeze(1)\n",
    "    death_time = torch.tensor(death_time_list).unsqueeze(1)\n",
    "    age = torch.tensor(age_list).unsqueeze(1)\n",
    "    ahi_diagnosis = torch.zeros_like(age)\n",
    "    #ahi_diagnosis = torch.tensor(ahi_diagnosis_list).unsqueeze(1)\n",
    "    \n",
    "    return x_data, y_data, padded_mask, hdf5_path_list, diagnosis_presence, diagnosis_time, death_presence, death_time, age, ahi_diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/oak/stanford/groups/jamesz/magnusrk/pretraining_comparison')\n",
    "from comparison.utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "#model classes\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_seq_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=1, dropout=0.1):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        batch_size, seq_len, input_dim = x.size()\n",
    "        \n",
    "        if key_padding_mask is not None:\n",
    "            if key_padding_mask.size(1) == 1:\n",
    "                return x.mean(dim=1)\n",
    "            if key_padding_mask.dtype != torch.bool:\n",
    "                key_padding_mask = key_padding_mask.to(dtype=torch.bool)\n",
    "                \n",
    "        transformer_output = self.transformer_layer(x, src_key_padding_mask=key_padding_mask)\n",
    "        pooled_output = transformer_output.mean(dim=1)  # Average pooling over the sequence length\n",
    "        \n",
    "        return pooled_output\n",
    "\n",
    "class SleepEventLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, num_classes, pooling_head=4, dropout=0.1, max_seq_length=128):\n",
    "        super(SleepEventLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # Define spatial pooling\n",
    "        #self.spatial_pooling = AttentionPooling(embed_dim, num_heads=pooling_head, dropout=dropout)\n",
    "\n",
    "        # Set max sequence length\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = 20000\n",
    "            \n",
    "        self.positional_encoding = PositionalEncoding(max_seq_length, embed_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Transformer encoder for spatial modeling\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # LSTM for temporal modeling\n",
    "        lstm_dropout = dropout if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=embed_dim//2, num_layers=num_layers, batch_first=True, dropout=lstm_dropout, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layer for sleep stage classification\n",
    "        self.fc_sleep_stage = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self.temporal_pooling = AttentionPooling(embed_dim, num_heads=pooling_head, dropout=dropout)\n",
    "\n",
    "        self.fc_age = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 1),\n",
    "            nn.Softplus()  # Ensures smooth, non-negative outputs\n",
    "        )\n",
    "\n",
    "        self.fc_ahi_diagnosis = nn.Linear(embed_dim, 1)\n",
    "\n",
    "        self.fc_death = nn.Linear(embed_dim, 1)\n",
    "\n",
    "        self.fc_diagnosis = nn.Linear(embed_dim, 12)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, S, E = x.shape       \n",
    "        device = x.device \n",
    "        #x = self.spatial_pooling(x, mask_spatial)\n",
    "\n",
    "        # Apply positional encoding and layer normalization\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # Apply transformer encoder for spatial modeling\n",
    "        mask_temporal = mask[:, :]\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask_temporal)\n",
    "\n",
    "        # Apply LSTM for temporal modeling\n",
    "        x, _ = self.lstm(x)  # Shape: (B, S, E)\n",
    "\n",
    "        # Apply the final fully connected layer for classification\n",
    "        sleep_stage = self.fc_sleep_stage(x)  # Shape: (B, S, num_classes)\n",
    "\n",
    "        \n",
    "        #x_diagnosis = self.temporal_pooling_diagnosis(x, mask_temporal)\n",
    "        #x_death = self.temporal_pooling_death(x, mask_temporal)\n",
    "        #x_age = self.temporal_pooling_age(x, mask_temporal)\n",
    "        x = self.temporal_pooling(x, mask_temporal)\n",
    "        hazards_death = self.fc_death(x)\n",
    "        hazards_diagnosis = self.fc_diagnosis(x)\n",
    "        age = self.fc_age(x)\n",
    "        ahi_diagnosis = self.fc_ahi_diagnosis(x)\n",
    "\n",
    "        return sleep_stage.to(device), mask[:, :].to(device), age.to(device), hazards_diagnosis.to(device), hazards_death.to(device), ahi_diagnosis.to(device)  # Return mask along temporal dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iteration(model, data, optimizer=None, scaler=None, config=None, device=None, mode='train'):\n",
    "    \"\"\"\n",
    "    Run one iteration (batch) of training or validation.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model\n",
    "        data: Tuple of batch data\n",
    "        optimizer: PyTorch optimizer (only needed for training)\n",
    "        scaler: Gradient scaler for mixed precision training\n",
    "        config: Configuration dictionary\n",
    "        device: PyTorch device\n",
    "        mode: Either 'train' or 'val'\n",
    "    \"\"\"\n",
    "    is_training = mode == 'train'\n",
    "    \n",
    "    # Unpack the batch data\n",
    "    x_data, y_data, mask, _, diagnosis_presence, diagnosis_time, death_presence, death_time, age_target, ahi_diagnosis_target = data\n",
    "    \n",
    "    # Move data to device\n",
    "    x_data = x_data.to(device)\n",
    "    y_data = y_data.to(device)\n",
    "    mask = mask.bool().to(device)\n",
    "    diagnosis_presence = diagnosis_presence.to(device)\n",
    "    diagnosis_time = diagnosis_time.to(device)\n",
    "    death_presence = death_presence.to(device)\n",
    "    death_time = death_time.to(device)\n",
    "    age_target = age_target.to(device)\n",
    "    ahi_diagnosis_target = ahi_diagnosis_target.to(device)\n",
    "\n",
    "    if is_training:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # Context manager for mixed precision training\n",
    "    with torch.cuda.amp.autocast() if is_training else torch.no_grad():\n",
    "        output, mask, age_out, hazards_diagnosis, hazards_death, ahi_diagnosis = model(x_data, mask)\n",
    "        \n",
    "        # Reshape outputs and targets\n",
    "        output_reshaped = output.reshape(-1, config['model_params']['num_classes'])\n",
    "        targets_reshaped = y_data.reshape(-1).long()\n",
    "        \n",
    "        # Handle masking\n",
    "        if mask is not None:\n",
    "            mask_reshaped = mask.reshape(-1)\n",
    "            valid_targets = targets_reshaped != -1\n",
    "            valid_mask = ~mask_reshaped & valid_targets\n",
    "            # Force contiguous memory layout before indexing\n",
    "\n",
    "            # If using DataParallel, ensure tensors are on the same device\n",
    "            if isinstance(model, torch.nn.DataParallel):\n",
    "                device = torch.device(f'cuda:{model.device_ids[0]}')\n",
    "                output_reshaped = output_reshaped.to(device)\n",
    "                valid_mask = valid_mask.to(device)\n",
    "\n",
    "            valid_mask = valid_mask.contiguous()\n",
    "            output_reshaped = output_reshaped.contiguous()\n",
    "            targets_reshaped = targets_reshaped.contiguous()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # if no valid targets set losses to 0 and return\n",
    "            if targets_reshaped.size(0) == 0:\n",
    "                loss = torch.tensor(0.0).to(device)\n",
    "                metrics = {\n",
    "                    'loss': loss.item(),\n",
    "                    'loss_sleep_staging': loss.item(),\n",
    "                    'loss_diagnosis': loss.item(),\n",
    "                    'loss_death': loss.item(),\n",
    "                    'loss_age': loss.item(),\n",
    "                    'loss_ahi_diagnosis': loss.item(),\n",
    "                    'correct': 0,\n",
    "                    'total': 0,\n",
    "                    'tp': torch.zeros(config['model_params']['num_classes']).to(device),\n",
    "                    'fp': torch.zeros(config['model_params']['num_classes']).to(device),\n",
    "                    'fn': torch.zeros(config['model_params']['num_classes']).to(device)\n",
    "                }\n",
    "                return metrics\n",
    "        \n",
    "        # Calculate losses\n",
    "        if mode == 'train':\n",
    "            loss_sleep_staging = torch.tensor(0.0, device=device, requires_grad=True)#masked_cross_entropy_loss(output, y_data, None)\n",
    "            loss_diagnosis = cox_ph_loss(hazards_diagnosis, diagnosis_time, diagnosis_presence)\n",
    "            loss_death = cox_ph_loss(hazards_death, death_time, death_presence)\n",
    "            loss_age = F.mse_loss(age_target.float(), age_out.float())\n",
    "            #loss_ahi_diagnosis = F.binary_cross_entropy_with_logits(ahi_diagnosis, ahi_diagnosis_target.float())\n",
    "            loss = loss_diagnosis + loss_death\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                loss_sleep_staging = torch.tensor(0.0, device=device)#masked_cross_entropy_loss(output, y_data, mask)\n",
    "                loss_diagnosis = cox_ph_loss(hazards_diagnosis, diagnosis_time, diagnosis_presence)\n",
    "                loss_death = cox_ph_loss(hazards_death, death_time, death_presence)\n",
    "                loss_age = F.mse_loss(age_target.float(), age_out.float())\n",
    "                #loss_ahi_diagnosis = F.binary_cross_entropy_with_logits(ahi_diagnosis, ahi_diagnosis_target.float())\n",
    "                loss = loss_diagnosis + loss_death\n",
    "\n",
    "    # Handle backpropagation for training\n",
    "    if is_training:\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # Calculate metrics\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(output_reshaped, 1)\n",
    "        total = targets_reshaped.size(0)\n",
    "        correct = (predicted == targets_reshaped).sum().item()\n",
    "        \n",
    "        # Calculate F1 components\n",
    "        tp = torch.zeros(config['model_params']['num_classes']).to(device)\n",
    "        fp = torch.zeros(config['model_params']['num_classes']).to(device)\n",
    "        fn = torch.zeros(config['model_params']['num_classes']).to(device)\n",
    "        \n",
    "        for class_idx in range(config['model_params']['num_classes']):\n",
    "            pred_mask = predicted == class_idx\n",
    "            target_mask = targets_reshaped == class_idx\n",
    "            \n",
    "            tp[class_idx] += (pred_mask & target_mask).sum()\n",
    "            fp[class_idx] += (pred_mask & ~target_mask).sum()\n",
    "            fn[class_idx] += (~pred_mask & target_mask).sum()\n",
    "    # Before returning, check for NaN values\n",
    "    metrics = {\n",
    "        'loss': loss.item(),\n",
    "        'loss_sleep_staging': loss_sleep_staging.item(),\n",
    "        'loss_diagnosis': loss_diagnosis.item(),\n",
    "        'loss_death': loss_death.item(),\n",
    "        'loss_age': loss_age.item(),\n",
    "        'loss_ahi_diagnosis': 0,#loss_ahi_diagnosis.item(),\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    }\n",
    "\n",
    "    # Check for NaN values\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, (float, int)):\n",
    "            if math.isnan(value):\n",
    "                print(f\"NaN detected in {key}\")\n",
    "                print(f\"Debug info:\")\n",
    "                print(f\"loss: {loss}\")\n",
    "                print(f\"loss_sleep_staging: {loss_sleep_staging}\")\n",
    "                print(f\"loss_diagnosis: {loss_diagnosis}\")\n",
    "                print(f\"loss_death: {loss_death}\")\n",
    "                print(f\"loss_age: {loss_age}\")\n",
    "                print(f\"y_data: {y_data}\")\n",
    "                print(f\"output: {output}\")\n",
    "                print(f\"valid_mask: {valid_mask}\")\n",
    "                print(f\"nan in y data: {torch.isnan(y_data).any()}\")\n",
    "                print(f\"nan in output: {torch.isnan(output).any()}\")\n",
    "                print(f\"nan in valid_mask: {torch.isnan(valid_mask).any()}\")\n",
    "                unique_targets_reshaped = torch.unique(targets_reshaped)\n",
    "                print(f\"unique targets: {unique_targets_reshaped}\")\n",
    "                unique_valid_mask = torch.unique(valid_mask)\n",
    "                print(f\"unique valid mask: {unique_valid_mask}\")\n",
    "                raise ValueError(f\"NaN detected in {key}\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, validation_loader, optimizer, scaler, config, device, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training metrics\n",
    "        train_metrics = {\n",
    "            'running_loss': 0.0,\n",
    "            'running_sleep_staging_loss': 0.0,\n",
    "            'running_diagnosis_loss': 0.0,\n",
    "            'running_death_loss': 0.0,\n",
    "            'running_age_loss': 0.0,\n",
    "            'running_ahi_diagnosis_loss': 0.0,\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'tp': torch.zeros(config['model_params']['num_classes']).to(device),\n",
    "            'fp': torch.zeros(config['model_params']['num_classes']).to(device),\n",
    "            'fn': torch.zeros(config['model_params']['num_classes']).to(device)\n",
    "        }\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_loop = tqdm(enumerate(train_loader), \n",
    "                            total=len(train_loader), \n",
    "                            desc=f'Epoch {epoch}/{config[\"epochs\"]-1}',\n",
    "                            leave=True,\n",
    "                            ncols=250)\n",
    "        \n",
    "        for i, batch_data in train_loop:\n",
    "            batch_metrics = run_iteration(model, batch_data, optimizer, scaler, config, device, mode='train')\n",
    "            \n",
    "            # Update running metrics\n",
    "            train_metrics['running_loss'] += batch_metrics['loss']\n",
    "            train_metrics['running_sleep_staging_loss'] += batch_metrics['loss_sleep_staging']\n",
    "            train_metrics['running_diagnosis_loss'] += batch_metrics['loss_diagnosis']\n",
    "            train_metrics['running_death_loss'] += batch_metrics['loss_death']\n",
    "            train_metrics['running_age_loss'] += batch_metrics['loss_age']\n",
    "            train_metrics['running_ahi_diagnosis_loss'] += batch_metrics['loss_ahi_diagnosis']\n",
    "            train_metrics['correct'] += batch_metrics['correct']\n",
    "            train_metrics['total'] += batch_metrics['total']\n",
    "            train_metrics['tp'] += batch_metrics['tp']\n",
    "            train_metrics['fp'] += batch_metrics['fp']\n",
    "            train_metrics['fn'] += batch_metrics['fn']\n",
    "\n",
    "            # Calculate current metrics for progress bar\n",
    "            batch_count = i + 1\n",
    "            avg_loss = train_metrics['running_loss'] / batch_count\n",
    "            accuracy = train_metrics['correct'] / train_metrics['total'] if train_metrics['total'] > 0 else 0\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            precision = train_metrics['tp'] / (train_metrics['tp'] + train_metrics['fp'] + 1e-7)\n",
    "            recall = train_metrics['tp'] / (train_metrics['tp'] + train_metrics['fn'] + 1e-7)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "            macro_f1 = f1.mean().item()\n",
    "\n",
    "            train_loop.set_postfix({\n",
    "                'loss': f'cur:{batch_metrics[\"loss\"]:.3f}/avg:{avg_loss:.3f}',\n",
    "                'sleep': f'cur:{batch_metrics[\"loss_sleep_staging\"]:.3f}/acc:{accuracy:.3f}/f1:{macro_f1:.3f}',\n",
    "                'diag': f'cur:{batch_metrics[\"loss_diagnosis\"]:.3f}',\n",
    "                'death': f'cur:{batch_metrics[\"loss_death\"]:.3f}',\n",
    "                'age': f'cur:{batch_metrics[\"loss_age\"]:.3f}',\n",
    "                'ahi': f'cur:{batch_metrics[\"loss_ahi_diagnosis\"]:.3f}/avg:{train_metrics[\"running_ahi_diagnosis_loss\"] / batch_count:.3f}'\n",
    "            })\n",
    "\n",
    "        # Validation loop\n",
    "        val_metrics = {\n",
    "            'running_loss': 0.0,\n",
    "            'running_sleep_staging_loss': 0.0,\n",
    "            'running_diagnosis_loss': 0.0,\n",
    "            'running_death_loss': 0.0,\n",
    "            'running_age_loss': 0.0,\n",
    "            'running_ahi_diagnosis_loss': 0.0,\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'tp': torch.zeros(config['model_params']['num_classes']).to(device),\n",
    "            'fp': torch.zeros(config['model_params']['num_classes']).to(device),\n",
    "            'fn': torch.zeros(config['model_params']['num_classes']).to(device)\n",
    "        }\n",
    "\n",
    "        model.eval()\n",
    "        val_loop = tqdm(enumerate(validation_loader), \n",
    "                        total=len(validation_loader), \n",
    "                        desc=f'Validation Epoch {epoch}/{config[\"epochs\"]-1}',\n",
    "                        leave=True,\n",
    "                        ncols=250)\n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in val_loop:\n",
    "                batch_metrics = run_iteration(model, batch_data, None, None, config, device, mode='val')\n",
    "                    \n",
    "                \n",
    "                # Update validation metrics\n",
    "                val_metrics['running_loss'] += batch_metrics['loss']\n",
    "                val_metrics['running_sleep_staging_loss'] += batch_metrics['loss_sleep_staging']\n",
    "                val_metrics['running_diagnosis_loss'] += batch_metrics['loss_diagnosis']\n",
    "                val_metrics['running_death_loss'] += batch_metrics['loss_death']\n",
    "                val_metrics['running_age_loss'] += batch_metrics['loss_age']\n",
    "                val_metrics['running_ahi_diagnosis_loss'] += batch_metrics['loss_ahi_diagnosis']\n",
    "                val_metrics['correct'] += batch_metrics['correct']\n",
    "                val_metrics['total'] += batch_metrics['total']\n",
    "                val_metrics['tp'] += batch_metrics['tp']\n",
    "                val_metrics['fp'] += batch_metrics['fp']\n",
    "                val_metrics['fn'] += batch_metrics['fn']\n",
    "\n",
    "                # Calculate current metrics\n",
    "                batch_count = i + 1\n",
    "                avg_val_loss = val_metrics['running_loss'] / batch_count\n",
    "                val_accuracy = val_metrics['correct'] / val_metrics['total'] if val_metrics['total'] > 0 else 0\n",
    "                \n",
    "                # Calculate F1 score\n",
    "                precision = val_metrics['tp'] / (val_metrics['tp'] + val_metrics['fp'] + 1e-7)\n",
    "                recall = val_metrics['tp'] / (val_metrics['tp'] + val_metrics['fn'] + 1e-7)\n",
    "                f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "                val_macro_f1 = f1.mean().item()\n",
    "\n",
    "                val_loop.set_postfix({\n",
    "                    'val_loss': f'cur:{batch_metrics[\"loss\"]:.3f}/avg:{avg_val_loss:.3f}',\n",
    "                    'sleep': f'cur:{batch_metrics[\"loss_sleep_staging\"]:.3f}/acc:{val_accuracy:.3f}/f1:{val_macro_f1:.3f}',\n",
    "                    'diag': f'cur:{batch_metrics[\"loss_diagnosis\"]:.3f}',\n",
    "                    'death': f'cur:{batch_metrics[\"loss_death\"]:.3f}',\n",
    "                    'age': f'cur:{batch_metrics[\"loss_age\"]:.3f}',\n",
    "                    'ahi': f'cur:{batch_metrics[\"loss_ahi_diagnosis\"]:.3f}/avg:{val_metrics[\"running_ahi_diagnosis_loss\"] / batch_count:.3f}'\n",
    "                })\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping trigger\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\nEarly stopping triggered after {epoch + 1} epochs')\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "        print(f'\\nEpoch {epoch} Summary: Training Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {macro_f1:.4f} Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_macro_f1:.4f} Best validation loss: {best_val_loss:.4f} Patience counter: {patience_counter}/{patience}')\n",
    "\n",
    "    print('\\nTraining finished!')\n",
    "    print(f'Best validation loss: {best_val_loss:.4f}') \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scaler, config, model_path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'config': config\n",
    "    }, model_path)\n",
    "    print(f'Model saved at {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save(model, test_loader, output_path, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and save predictions and targets.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        test_loader: DataLoader for test set\n",
    "        output_path: Path to save results\n",
    "        device: PyTorch device\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # For death-related metrics, we can use lists since there's one hazard per patient\n",
    "    death_hazards = []\n",
    "    death_time_list = []\n",
    "    death_presence_list = []\n",
    "    \n",
    "    # For diagnosis, we'll store complete batches to avoid flattening multiple hazards\n",
    "    diagnosis_batches = []\n",
    "    diagnosis_time_batches = []\n",
    "    diagnosis_presence_batches = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loop = tqdm(test_loader, desc='Evaluating', ncols=100)\n",
    "        \n",
    "        for x_data, y_data, mask, _, diagnosis_presence, diagnosis_time, death_presence, death_time, age_target, ahi_dignosis_target in test_loop:\n",
    "            # Move data to device\n",
    "            x_data = x_data.to(device)\n",
    "            y_data = y_data.to(device)\n",
    "            mask = mask.bool().to(device)\n",
    "            age_target = age_target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output, mask, age_out, diagnosis_hazard, death_hazard, ahi_diagnosis = model(x_data, mask)\n",
    "            \n",
    "            # Process sleep staging predictions\n",
    "            output_reshaped = output.reshape(-1, output.size(-1))\n",
    "            targets_reshaped = y_data.reshape(-1).long()\n",
    "            \n",
    "            # Apply masking\n",
    "            if mask is not None:\n",
    "                mask_reshaped = mask.reshape(-1)\n",
    "                valid_targets = targets_reshaped != -1\n",
    "                valid_mask = ~mask_reshaped & valid_targets\n",
    "                \n",
    "                output_reshaped = output_reshaped[valid_mask]\n",
    "                targets_reshaped = targets_reshaped[valid_mask]\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(output_reshaped, 1)\n",
    "            \n",
    "            # Store death-related predictions (can use extend since one hazard per patient)\n",
    "            death_hazards.extend(death_hazard.cpu().numpy().flatten().tolist())\n",
    "            death_time_list.extend(death_time.cpu().numpy().flatten().tolist())\n",
    "            death_presence_list.extend(death_presence.cpu().numpy().flatten().tolist())\n",
    "            \n",
    "            # Store diagnosis-related predictions as complete batches\n",
    "            diagnosis_batches.append(diagnosis_hazard.cpu().numpy())\n",
    "            diagnosis_time_batches.append(diagnosis_time.cpu().numpy())\n",
    "            diagnosis_presence_batches.append(diagnosis_presence.cpu().numpy())\n",
    "    \n",
    "    # Concatenate diagnosis batches along the batch dimension\n",
    "    diagnosis_hazards = np.concatenate(diagnosis_batches, axis=0)\n",
    "    diagnosis_times = np.concatenate(diagnosis_time_batches, axis=0)\n",
    "    diagnosis_presence = np.concatenate(diagnosis_presence_batches, axis=0)\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'death': {\n",
    "            'hazards': np.array(death_hazards),\n",
    "            'times': np.array(death_time_list),\n",
    "            'presence': np.array(death_presence_list)\n",
    "        },\n",
    "        'diagnosis': {\n",
    "            'hazards': diagnosis_hazards,  # This preserves the original shape with multiple hazards\n",
    "            'times': diagnosis_times,\n",
    "            'presence': diagnosis_presence\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save as numpy arrays\n",
    "    np.save(output_path, results)\n",
    "    print(f'Results saved to {output_path}')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def evaluate_val(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and save predictions and targets.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        test_loader: DataLoader for test set\n",
    "        device: PyTorch device\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # For death-related metrics, we can use lists since there's one hazard per patient\n",
    "    death_hazards = []\n",
    "    death_time_list = []\n",
    "    death_presence_list = []\n",
    "    \n",
    "    # For diagnosis, we'll store complete batches to avoid flattening multiple hazards\n",
    "    diagnosis_batches = []\n",
    "    diagnosis_time_batches = []\n",
    "    diagnosis_presence_batches = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loop = tqdm(val_loader, desc='Evaluating', ncols=100)\n",
    "        \n",
    "        for x_data, y_data, mask, _, diagnosis_presence, diagnosis_time, death_presence, death_time, age_target, ahi_dignosis_target in test_loop:\n",
    "            # Move data to device\n",
    "            x_data = x_data.to(device)\n",
    "            y_data = y_data.to(device)\n",
    "            mask = mask.bool().to(device)\n",
    "            age_target = age_target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output, mask, age_out, diagnosis_hazard, death_hazard, ahi_diagnosis = model(x_data, mask)\n",
    "            \n",
    "            # Process sleep staging predictions\n",
    "            output_reshaped = output.reshape(-1, output.size(-1))\n",
    "            targets_reshaped = y_data.reshape(-1).long()\n",
    "            \n",
    "            # Apply masking\n",
    "            if mask is not None:\n",
    "                mask_reshaped = mask.reshape(-1)\n",
    "                valid_targets = targets_reshaped != -1\n",
    "                valid_mask = ~mask_reshaped & valid_targets\n",
    "                \n",
    "                output_reshaped = output_reshaped[valid_mask]\n",
    "                targets_reshaped = targets_reshaped[valid_mask]\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(output_reshaped, 1)\n",
    "            \n",
    "            # Store death-related predictions (can use extend since one hazard per patient)\n",
    "            death_hazards.extend(death_hazard.cpu().numpy().flatten().tolist())\n",
    "            death_time_list.extend(death_time.cpu().numpy().flatten().tolist())\n",
    "            death_presence_list.extend(death_presence.cpu().numpy().flatten().tolist())\n",
    "            \n",
    "            # Store diagnosis-related predictions as complete batches\n",
    "            diagnosis_batches.append(diagnosis_hazard.cpu().numpy())\n",
    "            diagnosis_time_batches.append(diagnosis_time.cpu().numpy())\n",
    "            diagnosis_presence_batches.append(diagnosis_presence.cpu().numpy())\n",
    "    \n",
    "    # Concatenate diagnosis batches along the batch dimension\n",
    "    diagnosis_hazards = np.concatenate(diagnosis_batches, axis=0)\n",
    "    diagnosis_times = np.concatenate(diagnosis_time_batches, axis=0)\n",
    "    diagnosis_presence = np.concatenate(diagnosis_presence_batches, axis=0)\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'death': {\n",
    "            'hazards': np.array(death_hazards),\n",
    "            'times': np.array(death_time_list),\n",
    "            'presence': np.array(death_presence_list)\n",
    "        },\n",
    "        'diagnosis': {\n",
    "            'hazards': diagnosis_hazards,  # This preserves the original shape with multiple hazards\n",
    "            'times': diagnosis_times,\n",
    "            'presence': diagnosis_presence\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Calculate C-index for death\n",
    "    death_c_index = concordance_index(\n",
    "        results['death']['times'],\n",
    "        -results['death']['hazards'],  # Negative because higher hazard = lower survival time\n",
    "        results['death']['presence']\n",
    "    )\n",
    "    \n",
    "    # Calculate C-index for each diagnosis condition\n",
    "    diagnosis_c_indices = []\n",
    "    for i in range(results['diagnosis']['hazards'].shape[1]):  # For each condition\n",
    "        condition_c_index = concordance_index(\n",
    "            results['diagnosis']['times'][:, i],\n",
    "            -results['diagnosis']['hazards'][:, i],\n",
    "            results['diagnosis']['presence'][:, i]\n",
    "        )\n",
    "        diagnosis_c_indices.append(condition_c_index)\n",
    "    \n",
    "    # Calculate AUC for death\n",
    "    death_auc = roc_auc_score(\n",
    "        results['death']['presence'], \n",
    "        results['death']['hazards']\n",
    "    )\n",
    "    \n",
    "    # Calculate AUC for each diagnosis condition\n",
    "    diagnosis_aucs = []\n",
    "    for i in range(results['diagnosis']['hazards'].shape[1]):\n",
    "        condition_auc = roc_auc_score(\n",
    "            results['diagnosis']['presence'][:, i],\n",
    "            results['diagnosis']['hazards'][:, i]\n",
    "        )\n",
    "        diagnosis_aucs.append(condition_auc)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_c_index = np.mean([death_c_index] + diagnosis_c_indices)\n",
    "    avg_auc = np.mean([death_auc] + diagnosis_aucs)\n",
    "    \n",
    "    return avg_c_index, avg_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, validation_loader, optimizer, scaler, config, device, patience=10):\n",
    "    best_val_metric = 0  # Will store mean of AUC and C-index\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training metrics\n",
    "        train_metrics = {\n",
    "            'running_loss': 0.0,\n",
    "            'running_sleep_staging_loss': 0.0,\n",
    "            'running_diagnosis_loss': 0.0,\n",
    "            'running_death_loss': 0.0,\n",
    "            'running_age_loss': 0.0,\n",
    "            'running_ahi_diagnosis_loss': 0.0,\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'tp': torch.zeros(config['model_params']['num_classes']).to(device),\n",
    "            'fp': torch.zeros(config['model_params']['num_classes']).to(device),\n",
    "            'fn': torch.zeros(config['model_params']['num_classes']).to(device)\n",
    "        }\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_loop = tqdm(enumerate(train_loader), \n",
    "                         total=len(train_loader), \n",
    "                         desc=f'Epoch {epoch}/{config[\"epochs\"]-1}',\n",
    "                         leave=True,\n",
    "                         ncols=250)\n",
    "        \n",
    "        for i, batch_data in train_loop:\n",
    "            batch_metrics = run_iteration(model, batch_data, optimizer, scaler, config, device, mode='train')\n",
    "            \n",
    "            # Update running metrics\n",
    "            for key in train_metrics:\n",
    "                if key in batch_metrics:\n",
    "                    train_metrics[key] += batch_metrics[key]\n",
    "\n",
    "            # Calculate current metrics for progress bar\n",
    "            batch_count = i + 1\n",
    "            avg_loss = train_metrics['running_loss'] / batch_count\n",
    "            accuracy = train_metrics['correct'] / train_metrics['total'] if train_metrics['total'] > 0 else 0\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            precision = train_metrics['tp'] / (train_metrics['tp'] + train_metrics['fp'] + 1e-7)\n",
    "            recall = train_metrics['tp'] / (train_metrics['tp'] + train_metrics['fn'] + 1e-7)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "            macro_f1 = f1.mean().item()\n",
    "\n",
    "            train_loop.set_postfix({\n",
    "                'loss': f'cur:{batch_metrics[\"loss\"]:.3f}/avg:{avg_loss:.3f}',\n",
    "                'sleep': f'cur:{batch_metrics[\"loss_sleep_staging\"]:.3f}/acc:{accuracy:.3f}/f1:{macro_f1:.3f}',\n",
    "                'diag': f'cur:{batch_metrics[\"loss_diagnosis\"]:.3f}',\n",
    "                'death': f'cur:{batch_metrics[\"loss_death\"]:.3f}',\n",
    "                'age': f'cur:{batch_metrics[\"loss_age\"]:.3f}',\n",
    "                'ahi': f'cur:{batch_metrics[\"loss_ahi_diagnosis\"]:.3f}'\n",
    "            })\n",
    "\n",
    "        # Validation using evaluate_val\n",
    "        print(\"\\nRunning validation...\")\n",
    "        avg_val_c_index, avg_val_auc = evaluate_val(model, validation_loader, device)\n",
    "        \n",
    "        # Calculate combined metric for early stopping (mean of AUC and C-index)\n",
    "        current_val_metric = (avg_val_auc + avg_val_c_index) / 2\n",
    "\n",
    "        # Early stopping check\n",
    "        if current_val_metric > best_val_metric:\n",
    "            best_val_metric = current_val_metric\n",
    "            patience_counter = 0\n",
    "            # save best model as deep copy\n",
    "            best_model = deepcopy(model)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping trigger\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\nEarly stopping triggered after {epoch + 1} epochs')\n",
    "            #model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "        # Print epoch summary with detailed metrics\n",
    "        print(f'\\nEpoch {epoch} Summary:')\n",
    "        print(f'Training - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {macro_f1:.4f}')\n",
    "        print(f'Diagnosis - Avg AUC: {avg_val_auc:.4f}, Avg C-index: {avg_val_c_index:.4f}')\n",
    "        print(f'Death - AUC: {avg_val_auc:.4f}, C-index: {avg_val_c_index:.4f}')\n",
    "        print(f'Combined Metric (AUC+C-index)/2: {current_val_metric:.4f}')\n",
    "        print(f'Best validation metric so far: {best_val_metric:.4f}')\n",
    "        print(f'Patience counter: {patience_counter}/{patience}')\n",
    "\n",
    "    print('\\nTraining finished!')\n",
    "    print(f'Best validation metric (mean of AUC and C-index): {best_val_metric:.4f}')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = load_data('/oak/stanford/groups/jamesz/magnusrk/pretraining_comparison/fine_tune/config_fine_tune.yaml')\n",
    "config['batch_size'] = int(config['batch_size'])\n",
    "config['lr'] = config['lr'] / 1.5\n",
    "config['epochs'] = 20\n",
    "config['patience'] = 3\n",
    "config['wandb'] = False\n",
    "config['num_workers'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of available GPUs: 1\n",
      "Fine-tuning model with pretrain type: CL_pairwise_epochs_36 for death and diagnosis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oak/stanford/groups/mignot/abk26/conda_envs/SleepBench2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_19280/1889507795.py:60: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 0/19:   0%|                                                                                                                                                                                                                 | 0/681 [00:00<?, ?it/s]/tmp/ipykernel_19280/1839630913.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if is_training else torch.no_grad():\n",
      "Epoch 0/19: 100%|██████████████████████████████████████████████████████████████████████████| 681/681 [04:05<00:00,  2.77it/s, loss=cur:0.231/avg:0.000, sleep=cur:0.000/acc:0.094/f1:0.086, diag=cur:0.231, death=cur:0.000, age=cur:0.395, ahi=cur:0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████| 656/656 [01:58<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Summary:\n",
      "Training - Loss: 0.0000, Accuracy: 0.0942, F1: 0.0861\n",
      "Diagnosis - Avg AUC: 0.5841, Avg C-index: 0.6126\n",
      "Death - AUC: 0.5841, C-index: 0.6126\n",
      "Combined Metric (AUC+C-index)/2: 0.5983\n",
      "Best validation metric so far: 0.5983\n",
      "Patience counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/19:   0%|                                                                                                                                                                                                                 | 0/681 [00:00<?, ?it/s]/tmp/ipykernel_19280/1839630913.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if is_training else torch.no_grad():\n",
      "Epoch 1/19: 100%|██████████████████████████████████████████████████████████████████████████| 681/681 [03:29<00:00,  3.26it/s, loss=cur:0.000/avg:0.000, sleep=cur:0.000/acc:0.093/f1:0.081, diag=cur:0.000, death=cur:0.000, age=cur:0.145, ahi=cur:0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████| 656/656 [02:27<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training - Loss: 0.0000, Accuracy: 0.0931, F1: 0.0812\n",
      "Diagnosis - Avg AUC: 0.6433, Avg C-index: 0.6497\n",
      "Death - AUC: 0.6433, C-index: 0.6497\n",
      "Combined Metric (AUC+C-index)/2: 0.6465\n",
      "Best validation metric so far: 0.6465\n",
      "Patience counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/19:   0%|                                                                                                                                                                                                                 | 0/681 [00:00<?, ?it/s]/tmp/ipykernel_19280/1839630913.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if is_training else torch.no_grad():\n",
      "Epoch 2/19: 100%|██████████████████████████████████████████████████████████████████████████| 681/681 [03:40<00:00,  3.09it/s, loss=cur:0.102/avg:0.000, sleep=cur:0.000/acc:0.067/f1:0.072, diag=cur:0.102, death=cur:0.000, age=cur:0.176, ahi=cur:0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████| 656/656 [01:49<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training - Loss: 0.0000, Accuracy: 0.0673, F1: 0.0720\n",
      "Diagnosis - Avg AUC: 0.5713, Avg C-index: 0.6080\n",
      "Death - AUC: 0.5713, C-index: 0.6080\n",
      "Combined Metric (AUC+C-index)/2: 0.5897\n",
      "Best validation metric so far: 0.6465\n",
      "Patience counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/19:   0%|                                                                                                                                                                                                                 | 0/681 [00:00<?, ?it/s]/tmp/ipykernel_19280/1839630913.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if is_training else torch.no_grad():\n",
      "Epoch 3/19: 100%|██████████████████████████████████████████████████████████████████████████| 681/681 [03:23<00:00,  3.34it/s, loss=cur:0.097/avg:0.000, sleep=cur:0.000/acc:0.084/f1:0.077, diag=cur:0.097, death=cur:0.000, age=cur:0.123, ahi=cur:0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████| 656/656 [01:48<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training - Loss: 0.0000, Accuracy: 0.0842, F1: 0.0774\n",
      "Diagnosis - Avg AUC: 0.6269, Avg C-index: 0.6687\n",
      "Death - AUC: 0.6269, C-index: 0.6687\n",
      "Combined Metric (AUC+C-index)/2: 0.6478\n",
      "Best validation metric so far: 0.6478\n",
      "Patience counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/19:   0%|                                                                                                                                                                                                                 | 0/681 [00:00<?, ?it/s]/tmp/ipykernel_19280/1839630913.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if is_training else torch.no_grad():\n",
      "Epoch 4/19: 100%|██████████████████████████████████████████████████████████████████████████| 681/681 [03:23<00:00,  3.35it/s, loss=cur:0.882/avg:0.000, sleep=cur:0.000/acc:0.097/f1:0.082, diag=cur:0.882, death=cur:0.000, age=cur:0.154, ahi=cur:0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████| 656/656 [01:48<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training - Loss: 0.0000, Accuracy: 0.0973, F1: 0.0815\n",
      "Diagnosis - Avg AUC: 0.6483, Avg C-index: 0.6818\n",
      "Death - AUC: 0.6483, C-index: 0.6818\n",
      "Combined Metric (AUC+C-index)/2: 0.6650\n",
      "Best validation metric so far: 0.6650\n",
      "Patience counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/19:   0%|                                                                                                                                                                                                                 | 0/681 [00:00<?, ?it/s]/tmp/ipykernel_19280/1839630913.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if is_training else torch.no_grad():\n",
      "Epoch 5/19: 100%|██████████████████████████████████████████████████████████████████████████| 681/681 [03:31<00:00,  3.23it/s, loss=cur:0.302/avg:0.000, sleep=cur:0.000/acc:0.034/f1:0.017, diag=cur:0.302, death=cur:0.000, age=cur:0.388, ahi=cur:0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████| 656/656 [01:56<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training - Loss: 0.0000, Accuracy: 0.0336, F1: 0.0169\n",
      "Diagnosis - Avg AUC: 0.6403, Avg C-index: 0.6691\n",
      "Death - AUC: 0.6403, C-index: 0.6691\n",
      "Combined Metric (AUC+C-index)/2: 0.6547\n",
      "Best validation metric so far: 0.6650\n",
      "Patience counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/19:   0%|                                                                                                                                                                                                                 | 0/681 [00:00<?, ?it/s]/tmp/ipykernel_19280/1839630913.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if is_training else torch.no_grad():\n",
      "Epoch 6/19: 100%|██████████████████████████████████████████████████████████████████████████| 681/681 [04:00<00:00,  2.83it/s, loss=cur:0.903/avg:0.000, sleep=cur:0.000/acc:0.039/f1:0.035, diag=cur:0.903, death=cur:0.000, age=cur:0.228, ahi=cur:0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████| 656/656 [01:51<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training - Loss: 0.0000, Accuracy: 0.0389, F1: 0.0345\n",
      "Diagnosis - Avg AUC: 0.6139, Avg C-index: 0.6382\n",
      "Death - AUC: 0.6139, C-index: 0.6382\n",
      "Combined Metric (AUC+C-index)/2: 0.6261\n",
      "Best validation metric so far: 0.6650\n",
      "Patience counter: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/19:   0%|                                                                                                                                                                                                                 | 0/681 [00:00<?, ?it/s]/tmp/ipykernel_19280/1839630913.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if is_training else torch.no_grad():\n",
      "Epoch 7/19: 100%|██████████████████████████████████████████████████████████████████████████| 681/681 [04:03<00:00,  2.80it/s, loss=cur:0.095/avg:0.000, sleep=cur:0.000/acc:0.133/f1:0.102, diag=cur:0.095, death=cur:0.000, age=cur:0.291, ahi=cur:0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████| 656/656 [01:51<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping triggered after 8 epochs\n",
      "\n",
      "Training finished!\n",
      "Best validation metric (mean of AUC and C-index): 0.6650\n",
      "Model saved at /oak/stanford/groups/mignot/projects/SleepBenchTest/pretrain_comparison/output/results/CL_pairwise_epochs_36/death_and_diagnosis_model.pt\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                           | 0/656 [00:00<?, ?it/s]/oak/stanford/groups/mignot/abk26/conda_envs/SleepBench2/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "Evaluating: 100%|█████████████████████████████████████████████████| 656/656 [01:48<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: C-index: 0.6818, AUC: 0.6483 of the best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                            | 0/25 [00:00<?, ?it/s]/oak/stanford/groups/mignot/abk26/conda_envs/SleepBench2/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n",
      "Evaluating: 100%|███████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /oak/stanford/groups/mignot/projects/SleepBenchTest/pretrain_comparison/output/results/CL_pairwise_epochs_36/death_and_diagnosis_results.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace the model initialization section with:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available GPUs: {num_gpus}\")\n",
    "for pretrain_type in config['pretrain_type']:\n",
    "    print(f'Fine-tuning model with pretrain type: {pretrain_type} for death and diagnosis')\n",
    "    \n",
    "    \n",
    "    # Create datasets and dataloaders - note the increased batch size\n",
    "    train_dataset = SleepEventClassificationDataset(config, split=\"pretrain\", pretrain_type=pretrain_type)\n",
    "    validation_dataset = SleepEventClassificationDataset(config, split=\"validation\", pretrain_type=pretrain_type)\n",
    "    test_dataset = SleepEventClassificationDataset(config, split=\"test\", pretrain_type=pretrain_type)\n",
    "    \n",
    "    # Multiply batch size by number of GPUs since DataParallel splits it automatically\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=config['batch_size'], \n",
    "                            shuffle=True, \n",
    "                            num_workers=config['num_workers'], \n",
    "                            collate_fn=finetune_collate_fn,\n",
    "                            #pin_memory=True, \n",
    "                           drop_last=True)\n",
    "    \n",
    "    validation_loader = DataLoader(validation_dataset, \n",
    "                                 batch_size=(config['batch_size'] // 2), \n",
    "                                 shuffle=False, \n",
    "                                 num_workers=config['num_workers'], \n",
    "                                 collate_fn=finetune_collate_fn,\n",
    "                                 #pin_memory=True, \n",
    "                                 drop_last=True)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, \n",
    "                           batch_size=(config['batch_size']) // 2, \n",
    "                           shuffle=False, \n",
    "                           num_workers=config['num_workers'], \n",
    "                           collate_fn=finetune_collate_fn,\n",
    "                           #pin_memory=True, \n",
    "                           drop_last=True)\n",
    "\n",
    "    model = SleepEventLSTMClassifier(\n",
    "        embed_dim=config['model_params']['embed_dim'],\n",
    "        num_heads=config['model_params']['num_heads'],\n",
    "        num_layers=config['model_params']['num_layers'],\n",
    "        num_classes=config['model_params']['num_classes'],\n",
    "        pooling_head=config['model_params']['pooling_head'],\n",
    "        dropout=config['model_params']['dropout'],\n",
    "        max_seq_length=config['model_params']['max_seq_length']\n",
    "    )\n",
    "    \n",
    "    # Wrap model with DataParallel before moving to device\n",
    "    if num_gpus > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=[0, 1])  # Explicitly specify GPU devices\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Scale learning rate with number of GPUs\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    best_model = train(model, train_loader, validation_loader, optimizer, scaler, config, device, patience=config['patience'])\n",
    "    \n",
    "    #save_path = f'/scratch/users/magnusrk/pretraining_comparision/final_embeddings/{pretrain_type}/_death_and_diagnosis_model.pt'\n",
    "    save_path = os.path.join(config['save_path'], f'{pretrain_type}/death_and_diagnosis_model.pt')\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    save_model(best_model, optimizer, scaler, config, save_path)\n",
    "\n",
    "    print(\"\\nRunning validation...\")\n",
    "    avg_val_c_index, avg_val_auc = evaluate_val(best_model, validation_loader, device)\n",
    "    print(f'Validation: C-index: {avg_val_c_index:.4f}, AUC: {avg_val_auc:.4f} of the best model')\n",
    "\n",
    "    output_path = os.path.join(config['save_path'], f'{pretrain_type}/death_and_diagnosis_results.npy')\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    evaluate_and_save(best_model, test_loader, output_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nRunning validation...\")\n",
    "#avg_val_c_index, avg_val_auc = evaluate_val(best_model, validation_loader, device)\n",
    "#print(f'Validation: C-index: {avg_val_c_index:.4f}, AUC: {avg_val_auc:.4f} of the best model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_path = f'/oak/stanford/groups/jamesz/magnusrk/pretraining_comparison_data/diagnosis_results_v2/{pretrain_type}_death_and_diagnosis_model.npy'\n",
    "#evaluate_and_save(model, test_loader, output_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_path = f'/scratch/users/magnusrk/pretraining_comparision/final_embeddings/{pretrain_type}/lowLR_death_and_diagnosis_model.pt'\n",
    "#save_model(model, optimizer, scaler, config, save_path)\n",
    "\n",
    "#output_path = f'/oak/stanford/groups/jamesz/magnusrk/pretraining_comparison_data/diagnosis_results/{pretrain_type}_lowLR_death_and_diagnosis_model.npy'\n",
    "#evaluate_and_save(model, test_loader, output_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psg_fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
